# Story 3.2: Import du catalogue de sources curées

## Status: Done ✅

## Story

**As a** développeur,  
**I want** importer le catalogue de sources curées dans la base,  
**so that** tous les utilisateurs aient accès à du contenu de qualité dès le lancement.

## Acceptance Criteria

1. Script d'import Python depuis `sources/sources.csv` [Source: prd.md#Story 3.2] ✅
2. 24 sources initiales importées correctement [Source: sources.csv] ✅
3. Types correctement détectés et mappés vers les Enums (article, podcast, youtube) [Source: architecture.md#4.2] ✅
4. Thèmes assignés selon le CSV [Source: sources.csv] ✅
5. Flag `is_curated = true` pour toutes les sources importées ✅
6. Script réexécutable avec upsert (pas de doublons) [Source: prd.md#Story 3.2] ✅
7. Détection automatique du flux RSS pour chaque source ✅
8. **Critères FQS (Facteur Quality Score)** : Chaque source doit être évaluée selon 3 piliers (Indépendance, Rigueur, UX/Paywall) avant validation. [New]
9. **Mix de Pluralité** : Le catalogue doit inclure des sources à haute fréquence et des bords politiques variés (Droite/Libéral) pour équilibrer le feed. [New]

## Tasks / Subtasks

- [x] **Task 1: Créer le script d'import** (AC: 1, 2, 5, 6)
  - [x] Créer `packages/api/scripts/import_sources.py`
  - [x] Parser le CSV avec `csv.DictReader`
  - [x] Mapper les types CSV → `SourceType` enum (Site/Newsletter/RSS → article, Podcast → podcast, YouTube → youtube)
  - [x] Implémenter l'upsert via `ON CONFLICT (feed_url) DO UPDATE`
  - [x] Ajouter logging pour suivre l'import

- [x] **Task 2: Détection automatique des flux RSS** (AC: 7)
  - [x] Créer une fonction `detect_feed_url(url, source_type)` dans le script
  - [x] Pour YouTube: extraire le `channel_id` et construire l'URL RSS `https://www.youtube.com/feeds/videos.xml?channel_id=X`
  - [x] Pour les sites : tenter de trouver le flux RSS (balise `<link rel="alternate">`, ou `/feed`, `/rss`)
  - [x] Pour les podcasts : utiliser l'URL directement ou chercher le flux
  - [x] Stocker le `feed_url` détecté

- [x] **Task 3: Attribution des thèmes** (AC: 4)
  - [x] Mapper les thèmes CSV vers les slugs internes :
    - "Tech & Futur" → "tech"
    - "Géopolitique" → "geopolitics"
    - "Économie" → "economy"
    - "Société & Climat" → "society_climate"
    - "Culture & Idées" → "culture_ideas"

- [x] **Task 4: Tests et validation** (AC: 1-7)
  - [x] Lancer le script en mode dry-run pour vérifier
  - [x] Exécuter l'import réel et vérifier les 24 sources en base

## Dev Notes

### Architecture Context

#### Data Model - Source [Source: architecture.md#4.1, 4.2]

```python
class Source(Base):
    id: UUID
    name: str
    url: str
    feed_url: str  # URL du flux RSS (unique)
    type: SourceType  # article | podcast | youtube
    theme: str
    description: Optional[str]
    logo_url: Optional[str]
    is_curated: bool = True
    is_active: bool = True
    last_synced_at: Optional[datetime]
    created_at: datetime
```

#### Type Mapping

| Type CSV | SourceType Enum |
|----------|-----------------|
| Site | article |
| Newsletter | article |
| RSS | article |
| Podcast | podcast |
| YouTube | youtube |

#### Theme Mapping

| Thème CSV | Slug interne |
|-----------|--------------|
| Tech & Futur | tech |
| Géopolitique | geopolitics |
| Économie | economy |
| Société & Climat | society_climate |
| Culture & Idées | culture_ideas |

#### YouTube RSS Format [Source: architecture.md#6.3]

```
https://www.youtube.com/feeds/videos.xml?channel_id={CHANNEL_ID}
```

Pour extraire le `channel_id` depuis une URL YouTube :
- Handler `@channel` : faire une requête HTTP et parser le HTML pour trouver le canonical channel ID
- URL `/channel/UCxxxx` : extraire directement l'ID
- URL `/user/username` : faire une requête pour résoudre

### File Locations [Source: architecture.md#2.2]

- Script d'import : `packages/api/scripts/import_sources.py`
- Fichier source : `sources/sources.csv`
- Modèles : `packages/api/app/models/source.py`

### Strategy Context: FQS & Plurality [New]

#### Facteur Quality Score (FQS) Definition
Chaque source est scorée sur 100 pour garantir l'aspect "Slow Media" et qualitatif :
- **Indépendance & Transparence (40%)** : Propriété du média, influence actionnariale, clarté éditoriale.
- **Rigueur Journalistique (35%)** : Sourcing, vérification des faits, absence de clickbait.
- **Accessibilité & UX (25%)** : Impact des paywalls et de la publicité sur l'expérience de lecture.

#### Target Plurality Mix
Pour la pluralité, le catalogue doit viser à équilibrer les sources "Mainstream Deep" (Gauche/Centre) avec des sources de même rigueur sur le pôle Libéral/Conservateur (ex: L'Opinion, Commentaire).

### Previous Story Insights (Story 3.1)

- Les Enums `SourceType` sont définis dans `app/models/enums.py`
- Utiliser `native_enum=False` pour compatibilité PostgreSQL
- RLS policies à appliquer via `packages/api/sql/rls_policies.sql`

## Testing

- **Location**: `packages/api/tests/test_import_sources.py`
- **Framework**: pytest
- **Tests requis**:
  - Test du mapping type CSV → SourceType
  - Test du mapping thème CSV → slug
  - Test de l'extraction channel_id YouTube
  - Test de l'upsert (pas de doublons)

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 10/01/2026 | 1.0 | Initial Draft | BMad Master Agent |
