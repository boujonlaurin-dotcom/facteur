# Story 10.1 : Migration daily_top3 → daily_digest

## Status: Draft

## Story

**As a** développeur,  
**I want** migrer le modèle `daily_top3` vers `daily_digest` supportant 5 articles,  
**so that** le système puisse stocker et servir le nouveau format de digest quotidien.

## Acceptance Criteria

1. **Nouvelle table `daily_digest`** créée avec structure JSONB pour les items
2. **Migration Alembic** fonctionnelle (upgrade + downgrade)
3. **Modèle SQLAlchemy** `DailyDigest` créé avec les propriétés nécessaires
4. **Schéma Pydantic** `DigestItemSchema` et `DailyDigestSchema` créés
5. **Rétrocompatibilité** : L'ancien endpoint `/api/feed` continue de fonctionner (lecture briefing)
6. **Index** créé sur `(user_id, digest_date)` pour performance

## Tasks / Subtasks

### Backend

- [ ] **Task 1: Créer la migration Alembic**
  - [ ] Nouvelle table `daily_digest` avec colonnes :
    - `id` UUID PK
    - `user_id` UUID FK → auth.users
    - `digest_date` DATE NOT NULL
    - `generated_at` TIMESTAMPTZ DEFAULT NOW()
    - `items` JSONB NOT NULL DEFAULT '[]'
    - `total_items` INT DEFAULT 5
    - `items_actioned` INT DEFAULT 0
    - `is_complete` BOOLEAN DEFAULT FALSE
    - `completed_at` TIMESTAMPTZ
  - [ ] Contrainte UNIQUE sur `(user_id, digest_date)`
  - [ ] Index sur `(user_id, digest_date DESC)`
  - [ ] RLS policies pour sécurité

- [ ] **Task 2: Créer le modèle SQLAlchemy**
  - [ ] Fichier `packages/api/app/models/daily_digest.py`
  - [ ] Classe `DailyDigest` avec mapping complet
  - [ ] Méthodes helper : `add_item()`, `mark_item_actioned()`, `is_digest_complete()`
  - [ ] Export dans `__init__.py`

- [ ] **Task 3: Créer les schémas Pydantic**
  - [ ] `DigestItemSchema` : content_id, rank, status, digest_reason, actioned_at
  - [ ] `DailyDigestSchema` : id, user_id, digest_date, items[], progress, is_complete
  - [ ] `DigestItemStatus` Enum : pending, read, saved

- [ ] **Task 4: Conserver la compatibilité briefing**
  - [ ] L'endpoint `GET /api/feed` continue de fonctionner
  - [ ] Mapper `daily_digest` → format `briefing[]` existant si besoin

## Dev Notes

### Structure JSONB `items`

```json
[
  {
    "content_id": "uuid-1",
    "rank": 1,
    "status": "pending",
    "digest_reason": "Source suivie",
    "actioned_at": null
  },
  {
    "content_id": "uuid-2",
    "rank": 2,
    "status": "read",
    "digest_reason": "Sujet tendance",
    "actioned_at": "2026-01-31T08:15:00Z"
  }
]
```

### Schéma SQL complet

```sql
CREATE TABLE daily_digest (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
    digest_date DATE NOT NULL,
    generated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    
    items JSONB NOT NULL DEFAULT '[]',
    total_items INT NOT NULL DEFAULT 5,
    items_actioned INT NOT NULL DEFAULT 0,
    is_complete BOOLEAN NOT NULL DEFAULT FALSE,
    completed_at TIMESTAMPTZ,
    
    CONSTRAINT uq_daily_digest_user_date UNIQUE (user_id, digest_date)
);

CREATE INDEX idx_daily_digest_user_date ON daily_digest(user_id, digest_date DESC);

-- RLS
ALTER TABLE daily_digest ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can view own digest" ON daily_digest
    FOR SELECT USING (auth.uid() = user_id);

CREATE POLICY "Service can insert digest" ON daily_digest
    FOR INSERT WITH CHECK (true);

CREATE POLICY "Users can update own digest" ON daily_digest
    FOR UPDATE USING (auth.uid() = user_id);
```

### Modèle SQLAlchemy

```python
# packages/api/app/models/daily_digest.py

from datetime import date, datetime
from typing import Optional
from uuid import UUID
import enum

from sqlalchemy import Column, Date, DateTime, Integer, Boolean, ForeignKey, Index
from sqlalchemy.dialects.postgresql import UUID as PG_UUID, JSONB
from sqlalchemy.orm import relationship

from app.models.base import Base


class DigestItemStatus(str, enum.Enum):
    PENDING = "pending"
    READ = "read"
    SAVED = "saved"


class DailyDigest(Base):
    __tablename__ = "daily_digest"
    
    id = Column(PG_UUID(as_uuid=True), primary_key=True, server_default="gen_random_uuid()")
    user_id = Column(PG_UUID(as_uuid=True), ForeignKey("auth.users.id", ondelete="CASCADE"), nullable=False)
    digest_date = Column(Date, nullable=False)
    generated_at = Column(DateTime(timezone=True), server_default="now()", nullable=False)
    
    items = Column(JSONB, nullable=False, default=list)
    total_items = Column(Integer, nullable=False, default=5)
    items_actioned = Column(Integer, nullable=False, default=0)
    is_complete = Column(Boolean, nullable=False, default=False)
    completed_at = Column(DateTime(timezone=True), nullable=True)
    
    __table_args__ = (
        Index("idx_daily_digest_user_date", "user_id", "digest_date"),
    )
    
    def mark_item_actioned(self, content_id: UUID, status: DigestItemStatus) -> bool:
        """Marque un item comme actionné (read ou saved)."""
        for item in self.items:
            if item["content_id"] == str(content_id):
                if item["status"] == DigestItemStatus.PENDING.value:
                    item["status"] = status.value
                    item["actioned_at"] = datetime.utcnow().isoformat()
                    self.items_actioned += 1
                    
                    if self.items_actioned >= self.total_items:
                        self.is_complete = True
                        self.completed_at = datetime.utcnow()
                    
                    return True
        return False
    
    @property
    def progress(self) -> str:
        """Retourne la progression sous forme 'X/5'."""
        return f"{self.items_actioned}/{self.total_items}"
```

### Références Architecture

- [Source: architecture.md#data-models] — Patterns existants pour modèles
- [Source: 4.4.top3-briefing-quotidien.story.md] — Modèle `DailyTop3` existant

## Testing

- **Unit Tests** : `tests/models/test_daily_digest.py`
  - Test création digest
  - Test `mark_item_actioned()` 
  - Test détection `is_complete`
- **Migration** : Exécuter `alembic upgrade head` et vérifier schema

## Definition of Done

- [ ] Migration Alembic créée et testée (upgrade + downgrade)
- [ ] Modèle SQLAlchemy fonctionnel avec tests unitaires
- [ ] Schémas Pydantic créés
- [ ] RLS policies en place
- [ ] Aucune régression sur `/api/feed` existant

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 31/01/2026 | 1.0 | Création story | BMad Master |
