# Maintenance: RSS Storage Retention Policy

## Status & Metadata

- **Type**: Maintenance - Storage Management
- **Agent**: @dev
- **Date**: 2026-02-16
- **Phase**: ‚úÖ Implemented
- **Related**: `docs/maintenance/maintenance-capacity-analysis-alpha2.md` (√† cr√©er)

---

## Problem Statement

### Sympt√¥mes
- **Storage Supabase**: 411 MB / 500 MB (82.3%)
- **Utilisateurs actifs**: 10 users seulement
- **Projection**: Saturation compl√®te en 1-2 semaines
- **Goulot**: R√©tention illimit√©e des articles RSS (pas de purge automatique)

### Impact
- üî¥ **Critique**: Le backend bloquera √† 500 MB (Railway/Supabase hard limit)
- Nouveaux articles RSS ne pourront plus √™tre sync
- App mobile sera bloqu√©e (no new digests)
- N√©cessite intervention manuelle urgente pour √©viter downtime

---

## Root Cause Analysis

### Source du probl√®me
La table `contents` accumule tous les articles RSS synchro depuis le lancement:
- Sync RSS toutes les 30 minutes (configurable via `rss_sync_interval_minutes`)
- Aucune politique de r√©tention ‚Üí accumulation illimit√©e
- Articles de 2+ mois toujours en DB alors qu'invisibles dans l'app

### Donn√©es techniques
```sql
-- Articles par tranche d'√¢ge (estim√© pour 10 users)
SELECT
  COUNT(*) FILTER (WHERE published_at >= NOW() - INTERVAL '14 days') as recent_14d,
  COUNT(*) FILTER (WHERE published_at < NOW() - INTERVAL '14 days' AND published_at >= NOW() - INTERVAL '30 days') as old_14_30d,
  COUNT(*) FILTER (WHERE published_at < NOW() - INTERVAL '30 days') as ancient_30d_plus,
  COUNT(*) as total
FROM contents;

-- Attendu: 70-80% des articles ont > 14 jours (inutiles pour digests)
```

### Pourquoi 14 jours?
- **Digest quotidien**: Utilise uniquement articles r√©cents (< 7 jours en pratique)
- **Buffer de s√©curit√©**: 14 jours = 2x la fen√™tre active
- **User behavior**: Aucun user ne consulte articles > 2 semaines
- **Storage impact**: Purge 14j+ lib√®re ~150-200 MB (50% du storage)

---

## Solution

### Strat√©gie: Purge Automatique Quotidienne

Impl√©mentation d'un worker de nettoyage quotidien (cron 3 AM Paris) qui supprime les articles > 14 jours.

### Architecture
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ APScheduler (scheduler.py)                                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ RSS Sync (30 min interval)  ‚Üí Ajoute articles             ‚îÇ
‚îÇ ‚Ä¢ Daily Digest (8 AM)         ‚Üí Consomme articles r√©cents   ‚îÇ
‚îÇ ‚Ä¢ Storage Cleanup (3 AM)      ‚Üí Supprime articles > 14j  ‚ú® ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Impacts CASCADE (Foreign Keys)
La suppression d'un article `contents` d√©clenche CASCADE sur:
- ‚úÖ `user_content_status` (ondelete="CASCADE")
- ‚úÖ `daily_top3` (ondelete="CASCADE")
- ‚úÖ `classification_queue` (ondelete="CASCADE")
- ‚ö†Ô∏è `daily_digest.items` (JSONB) ‚Üí Orphelins mais **non bloquant** (digests pass√©s non affich√©s)

### Configuration
- **Env var**: `RSS_RETENTION_DAYS` (default: 14)
- **Runtime**: Modifiable sans red√©ploiement (config.py LRU cache)
- **Monitoring**: `verify_storage_health.sh` pour alerting

---

## Files Modified

### Created Files

1. **`packages/api/app/workers/storage_cleanup.py`** (73 lignes)
   - `async def cleanup_old_articles() -> dict`
   - Pattern A (simple async function) comme `rss_sync.py`
   - Logging structur√© (start, skip, completion, error)
   - Rollback automatique sur erreur

2. **`packages/api/tests/test_storage_cleanup.py`** (146 lignes)
   - 5 tests unitaires:
     - `test_cleanup_deletes_old_articles`
     - `test_cleanup_skips_when_no_old_articles`
     - `test_cleanup_rollback_on_error`
     - `test_cleanup_respects_custom_retention_days`
     - `test_cleanup_logs_statistics`

3. **`docs/qa/scripts/verify_storage_health.sh`** (44 lignes)
   - Query `pg_database_size(current_database())`
   - Breakdown articles (recent vs old)
   - Exit codes: 0 (OK <400MB), 1 (Warning 400-450MB), 2 (Critical >450MB)

### Modified Files

4. **`packages/api/app/config.py`** (+3 lignes)
   ```python
   # RSS Retention
   rss_retention_days: int = 14
   ```

5. **`packages/api/app/workers/scheduler.py`** (+10 lignes)
   - Import `cleanup_old_articles`
   - CronTrigger (3 AM Paris daily)
   - Job ID: `storage_cleanup`

---

## Verification

### 1. Tests unitaires
```bash
cd packages/api
source venv/bin/activate
pytest tests/test_storage_cleanup.py -v
```

**Attendu**: 5/5 tests passent

### 2. V√©rification scheduler
```bash
cd packages/api
uvicorn app.main:app --reload --port 8080
# Logs doivent montrer: "Scheduler started" avec 4 jobs (rss_sync, daily_top3, daily_digest, storage_cleanup)
```

### 3. Test manuel du worker
```python
# Python REPL ou script temporaire
import asyncio
from app.workers.storage_cleanup import cleanup_old_articles

async def test():
    result = await cleanup_old_articles()
    print(f"Deleted: {result['deleted_count']} articles")

asyncio.run(test())
```

### 4. Monitoring en production
```bash
# Supabase Dashboard ‚Üí Storage tab (before)
# Ex√©cuter purge manuelle (voir section suivante)
# Supabase Dashboard ‚Üí Storage tab (after)
# Attendu: -150 √† -200 MB

# Via script
DATABASE_URL="postgresql://..." bash docs/qa/scripts/verify_storage_health.sh
# Attendu: Storage < 300 MB (60%), exit code 0
```

---

## Purge Manuelle Imm√©diate (URGENT)

### Avant d√©ploiement du worker automatique

**Pourquoi**: Le worker cron s'ex√©cutera seulement demain √† 3 AM. Pour lib√©rer l'espace imm√©diatement, ex√©cuter cette requ√™te SQL dans Supabase SQL Editor.

### Requ√™te SQL (Supabase SQL Editor)

```sql
-- √âTAPE 1: V√©rifier le nombre d'articles √† supprimer (DRY RUN)
SELECT
    COUNT(*) as articles_to_delete,
    MIN(published_at) as oldest_article,
    MAX(published_at) as newest_article_to_delete
FROM contents
WHERE published_at < NOW() - INTERVAL '14 days';

-- Attendu: ~5000-8000 articles (estimation pour 10 users)

-- √âTAPE 2: V√©rifier l'espace avant purge
SELECT pg_database_size(current_database()) / 1024 / 1024 AS size_before_mb;

-- Attendu: ~411 MB

-- √âTAPE 3: Ex√©cuter la purge (ATTENTION: Action irr√©versible)
-- ‚ö†Ô∏è Les FK CASCADE supprimeront aussi user_content_status, daily_top3, classification_queue
DELETE FROM contents
WHERE published_at < NOW() - INTERVAL '14 days';

-- √âTAPE 4: V√©rifier l'espace apr√®s purge
SELECT pg_database_size(current_database()) / 1024 / 1024 AS size_after_mb;

-- Attendu: ~250-300 MB (baisse de 150-200 MB)

-- √âTAPE 5: V√©rifier la r√©partition des articles restants
SELECT
    COUNT(*) as remaining_articles,
    MIN(published_at) as oldest_remaining,
    MAX(published_at) as newest_article
FROM contents;

-- Attendu: ~2000-3000 articles r√©cents (< 14 jours)
```

### Screenshots √† capturer
- **Avant**: Supabase Dashboard ‚Üí Storage ‚Üí 411 MB / 500 MB
- **Apr√®s**: Supabase Dashboard ‚Üí Storage ‚Üí ~260 MB / 500 MB

---

## Rollback Plan

### Si probl√®me d√©tect√© apr√®s d√©ploiement

1. **D√©sactiver le worker** (sans red√©ploiement):
   ```bash
   # Railway Variables
   RSS_RETENTION_DAYS=999999  # Emp√™che purge (articles trop vieux n'existent pas)
   ```

2. **Rollback code** (si bugs critiques):
   ```bash
   git revert <commit-sha>
   git push origin main
   # Railway auto-red√©ploie
   ```

3. **Restaurer depuis backup Supabase** (si perte de donn√©es critique):
   - Supabase Dashboard ‚Üí Database ‚Üí Backups
   - Restaurer snapshot pr√©-purge (Supabase garde 7 jours de backups)

---

## Prevention & Monitoring

### Alerting
```bash
# Cron quotidien (Railway/Github Actions)
0 9 * * * DATABASE_URL=$DATABASE_URL bash docs/qa/scripts/verify_storage_health.sh || curl -X POST $SLACK_WEBHOOK -d '{"text":"‚ö†Ô∏è Storage critique"}'
```

### M√©triques √† surveiller
- **Storage usage**: Doit rester stable √† 250-300 MB apr√®s purge
- **Article count**: ~2000-3000 articles (< 14 jours)
- **Worker logs**: `storage_cleanup_completed` quotidien dans logs Railway
- **Erreurs**: Aucune erreur `storage_cleanup_failed` dans Sentry

### Guardrails
- **Index `ix_contents_published_at`**: DELETE WHERE est rapide (< 5 secondes)
- **FK CASCADE**: PostgreSQL g√®re les suppressions d√©pendantes atomiquement
- **Rollback automatique**: Exception dans worker ‚Üí session.rollback() ‚Üí aucune perte partielle

---

## Next Steps (Post-Implementation)

1. ‚úÖ D√©ployer sur Railway (commit + push)
2. ‚è≥ Ex√©cuter purge manuelle SQL (AVANT d√©ploiement pour espace imm√©diat)
3. ‚è≥ V√©rifier logs Railway apr√®s d√©ploiement (scheduler d√©marre avec 4 jobs)
4. ‚è≥ Attendre 3 AM Paris (1√®re ex√©cution auto du worker)
5. ‚è≥ V√©rifier logs le lendemain: `storage_cleanup_completed` avec `deleted_count`
6. ‚è≥ Cr√©er `maintenance-capacity-analysis-alpha2.md` (analyse compl√®te capacit√©)
7. ‚è≥ Update CHANGELOG.md avec cette maintenance

---

## Related Documentation

- **Capacity Analysis**: `docs/maintenance/maintenance-capacity-analysis-alpha2.md` (√† cr√©er)
- **Architecture**: `docs/architecture.md` (Workers section)
- **Safety Guardrails**: `docs/agent-brain/safety-guardrails.md` (DB operations)
- **Navigation Matrix**: `docs/agent-brain/navigation-matrix.md` (Maintenance workflow)

---

*Maintenance compl√©t√©e par: @dev agent (Claude Code)*
*Date d'impl√©mentation: 2026-02-16*
*Status: ‚úÖ Code ready, ‚è≥ Awaiting deployment + manual purge*
