---
phase: 01-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - packages/api/sql/009_daily_digest_table.sql
  - packages/api/sql/010_digest_completions_table.sql
  - packages/api/sql/011_extend_user_streaks.sql
  - packages/api/app/models/daily_digest.py
  - packages/api/app/models/digest_completion.py
  - packages/api/app/models/__init__.py
  - packages/api/app/models/user.py
autonomous: true

must_haves:
  truths:
    - "daily_digest table exists with JSONB items array for 5 articles"
    - "digest_completions table tracks user daily completions"
    - "user_streaks table extended with closure tracking fields"
    - "All SQL migrations are idempotent (IF NOT EXISTS / IF NOT EXISTS ADD COLUMN)"
  artifacts:
    - path: "packages/api/sql/009_daily_digest_table.sql"
      provides: "Migration for daily_digest table (replaces daily_top3 pattern)"
      contains: "CREATE TABLE daily_digest, JSONB items column"
    - path: "packages/api/sql/010_digest_completions_table.sql"
      provides: "Migration for digest_completions table"
      contains: "CREATE TABLE digest_completions, user_id, date, completed_at"
    - path: "packages/api/sql/011_extend_user_streaks.sql"
      provides: "Migration to extend user_streaks table"
      contains: "ADD COLUMN closure_streak, longest_closure_streak, last_closure_date"
    - path: "packages/api/app/models/daily_digest.py"
      provides: "SQLAlchemy model for daily_digest"
      contains: "class DailyDigest with items JSONB field"
    - path: "packages/api/app/models/digest_completion.py"
      provides: "SQLAlchemy model for digest_completions"
      contains: "class DigestCompletion"
  key_links:
    - from: "daily_digest.items"
      to: "content table"
      via: "JSONB array with content_id references"
      pattern: "items[].content_id references contents.id"
---

<objective>
Create database schema for the digest system.

Purpose: Provide the data layer for Epic 10's digest-first experience, replacing the Top 3 pattern with a 5-article digest model.

Output: 
- 3 SQL migration files (009, 010, 011)
- 2 new SQLAlchemy models (daily_digest, digest_completion)
- Extended user_streaks model with closure tracking
</objective>

<execution_context>
@/Users/laurinboujon/.config/opencode/get-shit-done/workflows/execute-plan.md
@/Users/laurinboujon/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/REQUIREMENTS.md

# Existing models to reference
@packages/api/app/models/daily_top3.py
@packages/api/app/models/user.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create daily_digest table migration</name>
  <files>packages/api/sql/009_daily_digest_table.sql, packages/api/app/models/daily_digest.py</files>
  <action>
Create SQL migration 009 for daily_digest table and corresponding SQLAlchemy model.

SQL Migration (009_daily_digest_table.sql):
- Create table `daily_digest` with:
  - id (UUID primary key, default gen_random_uuid())
  - user_id (UUID, not null, foreign key to users.id)
  - date (DATE, not null) — the date of the digest
  - items (JSONB, not null) — array of 5 articles with content_id, rank, reason, action_status
  - generated_at (TIMESTAMPTZ, default now())
  - completed (BOOLEAN, default false)
  - completed_at (TIMESTAMPTZ, nullable)
- Add unique constraint on (user_id, date)
- Add index on user_id + date
- Add GIN index on items for JSONB queries

SQLAlchemy Model (app/models/daily_digest.py):
- Create class DailyDigest(Base) following the pattern in daily_top3.py
- Use Mapped[] annotations for type hints
- Include __tablename__, __table_args__ for indexes/constraints
- Import at app/models/__init__.py
- The items JSONB should store: [{"content_id": "uuid", "rank": 1, "reason": "...", "action": "unread|read|saved|not_interested"}, ...]

Idempotency requirement: Use "CREATE TABLE IF NOT EXISTS" and handle index creation safely.
  </action>
  <verify>
psql $DATABASE_URL -f packages/api/sql/009_daily_digest_table.sql
# Verify table exists:
psql $DATABASE_URL -c "\dt daily_digest"
# Verify model loads:
cd packages/api && python -c "from app.models.daily_digest import DailyDigest; print('OK')"
  </verify>
  <done>
Migration 009 runs without errors, table exists in Supabase, model imports successfully
  </done>
</task>

<task type="auto">
  <name>Task 2: Create digest_completions table migration</name>
  <files>packages/api/sql/010_digest_completions_table.sql, packages/api/app/models/digest_completion.py</files>
  <action>
Create SQL migration 010 for digest_completions table and corresponding model.

SQL Migration (010_digest_completions_table.sql):
- Create table `digest_completions` with:
  - id (UUID primary key)
  - user_id (UUID, not null, foreign key to users.id, cascade delete)
  - date (DATE, not null) — completion date
  - completed_at (TIMESTAMPTZ, default now())
  - time_to_closure_minutes (INTEGER, nullable) — how long user took to finish
  - source_count_at_completion (INTEGER, nullable) — how many sources user followed at time of completion
- Add unique constraint on (user_id, date)
- Add index on user_id + date

SQLAlchemy Model (app/models/digest_completion.py):
- Create class DigestCompletion(Base) following existing patterns
- Add to app/models/__init__.py

Idempotency: Use "CREATE TABLE IF NOT EXISTS".
  </action>
  <verify>
psql $DATABASE_URL -f packages/api/sql/010_digest_completions_table.sql
psql $DATABASE_URL -c "\dt digest_completions"
cd packages/api && python -c "from app.models.digest_completion import DigestCompletion; print('OK')"
  </verify>
  <done>
Migration 010 runs without errors, table exists, model imports successfully
  </done>
</task>

<task type="auto">
  <name>Task 3: Extend user_streaks with closure tracking</name>
  <files>packages/api/sql/011_extend_user_streaks.sql, packages/api/app/models/user.py</files>
  <action>
Create migration 011 to add closure streak fields to existing user_streaks table.

SQL Migration (011_extend_user_streaks.sql):
- Add columns to existing `user_streaks` table:
  - closure_streak (INTEGER, default 0) — consecutive days of digest completion
  - longest_closure_streak (INTEGER, default 0) — personal best
  - last_closure_date (DATE, nullable) — last date user completed a digest
- Add index on (user_id, last_closure_date) for streak calculations

Update SQLAlchemy Model (app/models/user.py):
- Add the three new columns to UserStreak class
- Follow existing pattern with Mapped[] annotations
- Update __table_args__ if adding new indexes

Idempotency: Use "ALTER TABLE ... ADD COLUMN IF NOT EXISTS" for each column.

Verify existing streak logic continues working after these additions.
  </action>
  <verify>
psql $DATABASE_URL -f packages/api/sql/011_extend_user_streaks.sql
psql $DATABASE_URL -c "\d user_streaks" | grep -E "(closure|longest|last_closure)"
cd packages/api && python -c "from app.models.user import UserStreak; s = UserStreak(); print('closure_streak' in dir(s))"
  </verify>
  <done>
Migration 011 runs without errors, all three columns added, existing streak_service.py still works
  </done>
</task>

</tasks>

<verification>
1. All three SQL migrations are idempotent and run successfully
2. Tables appear in Supabase with correct structure
3. All three SQLAlchemy models import without errors
4. Existing streak service continues functioning (no regression)
5. Database relationships are properly defined (foreign keys, cascade)
</verification>

<success_criteria>
- daily_digest table exists with JSONB items array for 5 articles
- digest_completions table tracks daily completions with metadata
- user_streaks extended with closure_streak, longest_closure_streak, last_closure_date
- All migrations are idempotent (safe to re-run)
- Models integrate cleanly with FastAPI/SQLAlchemy async session
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-01-SUMMARY.md`
</output>
